<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sabari Nathan - Computer Vision Research Engineer</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Roboto+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Roboto', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: #202124;
            background-color: #ffffff;
            font-size: 16px;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 0 24px;
        }

        /* Header */
        header {
            background: #ffffff;
            border-bottom: 1px solid #e8eaed;
            position: sticky;
            top: 0;
            z-index: 100;
        }

        nav {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 16px 0;
        }

        .logo {
            font-size: 20px;
            font-weight: 500;
            color: #202124;
            text-decoration: none;
        }

        .nav-links {
            display: flex;
            list-style: none;
            gap: 32px;
        }

        .nav-links a {
            text-decoration: none;
            color: #5f6368;
            font-weight: 400;
            transition: color 0.2s ease;
            font-size: 14px;
        }

        .nav-links a:hover {
            color: #1a73e8;
        }

        /* Main Content */
        main {
            padding: 48px 0;
        }

        /* Profile Section */
        .profile-section {
            display: grid;
            grid-template-columns: 200px 1fr;
            gap: 48px;
            margin-bottom: 64px;
            align-items: start;
        }

        .profile-image-container {
            position: relative;
        }

        .profile-image {
            width: 200px;
            height: 200px;
            border-radius: 8px;
            background: #f8f9fa;
            border: 1px solid #e8eaed;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #5f6368;
            font-size: 14px;
            text-align: center;
            padding: 20px;
        }

        .profile-content h1 {
            font-size: 48px;
            font-weight: 400;
            margin-bottom: 8px;
            color: #202124;
            letter-spacing: -0.5px;
        }

        .profile-content .subtitle {
            font-size: 20px;
            color: #5f6368;
            margin-bottom: 24px;
            font-weight: 300;
        }

        .profile-content .affiliation {
            font-size: 16px;
            color: #5f6368;
            margin-bottom: 24px;
        }

        .profile-links {
            display: flex;
            gap: 24px;
            margin-bottom: 24px;
            flex-wrap: wrap;
        }

        .profile-link {
            color: #1a73e8;
            text-decoration: none;
            font-weight: 400;
            padding: 8px 0;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
            font-size: 14px;
        }

        .profile-link:hover {
            border-bottom-color: #1a73e8;
        }

        /* Section Styling */
        .section {
            margin-bottom: 64px;
        }

        .section-title {
            font-size: 32px;
            font-weight: 400;
            margin-bottom: 32px;
            color: #202124;
            letter-spacing: -0.25px;
        }

        .research-description {
            font-size: 16px;
            line-height: 1.7;
            color: #3c4043;
            margin-bottom: 24px;
            max-width: 800px;
        }

        .research-interests {
            background: #f8f9fa;
            padding: 24px;
            border-radius: 8px;
            margin-bottom: 32px;
            border-left: 4px solid #1a73e8;
        }

        /* Publications */
        .publications-grid {
            display: grid;
            gap: 32px;
        }

        .publication {
            background: #ffffff;
            border: 1px solid #e8eaed;
            border-radius: 8px;
            padding: 24px;
            transition: box-shadow 0.2s ease;
        }

        .publication:hover {
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .publication-header {
            display: flex;
            align-items: flex-start;
            gap: 20px;
            margin-bottom: 16px;
        }

        .publication-image {
            width: 120px;
            height: 80px;
            background: #f8f9fa;
            border: 1px solid #e8eaed;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: #5f6368;
            font-size: 12px;
            text-align: center;
            flex-shrink: 0;
        }

        .publication-info {
            flex: 1;
        }

        .publication-title {
            font-size: 18px;
            font-weight: 500;
            margin-bottom: 8px;
            color: #1a73e8;
            line-height: 1.4;
            cursor: pointer;
        }

        .publication-title:hover {
            text-decoration: underline;
        }

        .publication-authors {
            color: #5f6368;
            margin-bottom: 4px;
            font-size: 14px;
        }

        .publication-venue {
            color: #ea4335;
            font-weight: 500;
            margin-bottom: 12px;
            font-size: 14px;
        }

        .publication-description {
            color: #3c4043;
            line-height: 1.6;
            font-size: 14px;
        }

        .publication-links {
            margin-top: 12px;
            display: flex;
            gap: 16px;
        }

        .publication-link {
            color: #1a73e8;
            text-decoration: none;
            font-size: 12px;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        .publication-link:hover {
            text-decoration: underline;
        }

        /* News Section */
        .news-grid {
            display: grid;
            gap: 16px;
            max-width: 800px;
        }

        .news-item {
            padding: 16px 0;
            border-bottom: 1px solid #e8eaed;
            display: flex;
            gap: 24px;
            align-items: flex-start;
        }

        .news-item:last-child {
            border-bottom: none;
        }

        .news-date {
            font-size: 14px;
            color: #5f6368;
            font-weight: 500;
            min-width: 100px;
            font-family: 'Roboto Mono', monospace;
        }

        .news-content {
            color: #3c4043;
            font-size: 14px;
            line-height: 1.5;
        }

        /* Academic Service */
        .service-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 32px;
        }

        .service-card {
            background: #f8f9fa;
            padding: 24px;
            border-radius: 8px;
        }

        .service-card h3 {
            color: #202124;
            margin-bottom: 16px;
            font-size: 18px;
            font-weight: 500;
        }

        .service-list {
            list-style: none;
        }

        .service-list li {
            padding: 4px 0;
            color: #3c4043;
            font-size: 14px;
            position: relative;
            padding-left: 16px;
        }

        .service-list li:before {
            content: "‚Ä¢";
            color: #5f6368;
            position: absolute;
            left: 0;
            top: 4px;
        }

        /* Contact Section */
        .contact-info {
            background: #f8f9fa;
            padding: 32px;
            border-radius: 8px;
            text-align: center;
            max-width: 600px;
            margin: 0 auto;
        }

        .contact-info h3 {
            color: #202124;
            margin-bottom: 16px;
            font-size: 20px;
            font-weight: 400;
        }

        .contact-info p {
            color: #3c4043;
            margin-bottom: 8px;
            font-size: 14px;
        }

        .contact-email {
            color: #1a73e8;
            text-decoration: none;
            font-weight: 500;
        }

        .contact-email:hover {
            text-decoration: underline;
        }

        /* Footer */
        footer {
            background: #f8f9fa;
            border-top: 1px solid #e8eaed;
            padding: 32px 0;
            margin-top: 64px;
            text-align: center;
        }

        .footer-note {
            font-size: 12px;
            color: #5f6368;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 0 16px;
            }

            .profile-section {
                grid-template-columns: 1fr;
                text-align: center;
                gap: 24px;
            }

            .profile-image {
                width: 150px;
                height: 150px;
                margin: 0 auto;
            }

            .profile-content h1 {
                font-size: 36px;
            }

            .nav-links {
                gap: 16px;
            }

            .service-grid {
                grid-template-columns: 1fr;
            }

            .publication-header {
                flex-direction: column;
                gap: 16px;
            }

            .publication-image {
                width: 100%;
                height: 120px;
            }

            .news-item {
                flex-direction: column;
                gap: 8px;
            }

            .news-date {
                min-width: auto;
            }
        }

        /* Animation */
        .publication {
            opacity: 0;
            transform: translateY(20px);
            animation: fadeInUp 0.6s ease forwards;
        }

        .publication:nth-child(1) { animation-delay: 0.1s; }
        .publication:nth-child(2) { animation-delay: 0.2s; }
        .publication:nth-child(3) { animation-delay: 0.3s; }
        .publication:nth-child(4) { animation-delay: 0.4s; }

        @keyframes fadeInUp {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>
<body>
    <header>
        <nav class="container">
            <a href="#" class="logo">Sabari Nathan</a>
            <ul class="nav-links">
                <li><a href="#research">Research</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="#service">Service</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <!-- Profile Section -->
        <section class="profile-section">
            <div class="profile-image-container">
                <div class="profile-image">
                    <img src="images/pic.jpg" alt="Sabari Nathan" 
                         style="width: 100%; height: 100%; object-fit: cover; border-radius: 8px;">
                </div>
            </div>
            <div class="profile-content">
                <h1>Sabari Nathan</h1>
                <p class="subtitle">Computer Vision Research Engineer</p>
                <p class="affiliation">Deep Learning & Computer Vision Specialist</p>
                <div class="profile-links">
                    <a href="mailto:your.email@domain.com" class="profile-link">Email</a>
                    <a href="https://scholar.google.com/citations?user=3pySUPQAAAAJ&hl=en" class="profile-link">Google Scholar</a>
                    <a href="https://github.com/dsabarinathan" class="profile-link">GitHub</a>
                    <a href="https://linkedin.com/in/yourprofile" class="profile-link">LinkedIn</a>
                    <a href="/path/to/your/cv.pdf" class="profile-link">CV</a>
                </div>
                <p class="research-description">
                    <strong>Computer Vision Research Engineer</strong> with <strong>10+ years of experience</strong> in deep learning, cultural heritage preservation, 
                    and practical AI applications. Published <strong>25+ papers</strong> in top-tier conferences and journals including <strong>CVPR</strong>, 
                    <strong>ECCV</strong>, <strong>Heritage Science</strong>, and <strong>Journal of Food Engineering</strong>. <strong>Proven competition champion</strong> 
                    with <strong>1st place victory</strong> in SSBC 2025 and multiple top-3 finishes in prestigious computer vision challenges including 
                    <strong>CVPR competitions</strong> (thermal super-resolution, skeleton segmentation), <strong>Facebook OpenEDS</strong>, and <strong>Kaggle Gold Medal</strong> (Top 1%). 
                    Expertise spans <strong>face anti-spoofing systems</strong>, <strong>heritage image classification</strong>, <strong>food technology AI</strong>, 
                    <strong>UAV-based infrastructure monitoring</strong>, and <strong>mobile camera enhancement</strong>. Strong focus on creating 
                    <strong>real-world impact</strong> through interdisciplinary research spanning computer vision, cultural studies, and industrial applications.
                </p>
            </div>
        </section>

        <!-- Research Section -->
        <section id="research" class="section">
            <h2 class="section-title">Research</h2>
            <div class="research-interests">
                <p>
                    My research spans multiple domains of computer vision and AI applications, with a strong focus on <strong>cultural heritage preservation</strong>, 
                    <strong>practical AI systems</strong>, and <strong>real-world problem solving</strong>. I specialize in developing deep learning solutions for 
                    <strong>heritage image classification</strong> (Kolkata monuments, Tamil Kolam art), <strong>face anti-spoofing and security systems</strong>, 
                    <strong>food technology and agricultural applications</strong> (freshness prediction, medicinal plant identification), 
                    <strong>transportation infrastructure monitoring</strong> (UAV-based railroad analysis), and <strong>mobile camera enhancement technologies</strong>. 
                    My work emphasizes bridging academic research with industrial applications and cultural preservation.
                </p>
            </div>
            <p class="research-description">
                <em>How can computer vision technologies solve diverse real-world challenges while preserving cultural heritage and advancing practical AI applications across multiple industries?</em>
            </p>
        </section>

        <!-- Publications Section -->
        <section id="publications" class="section">
            <h2 class="section-title">Publications</h2>
            <div class="publications-grid">
                
                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Face Anti-Spoofing]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Multiattention-Net: A Novel Approach to Face Anti-Spoofing with Modified Squeezed Residual Blocks</h3>
                            <p class="publication-authors">Sabari Nathan, M Parisa Beham, A Nagaraj, S Roomi</p>
                            <p class="publication-venue">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</p>
                            <p class="publication-description">
                                We present Multiattention-Net, a novel deep learning architecture for face anti-spoofing that combines attention mechanisms 
                                with modified squeezed residual blocks. Our approach achieves state-of-the-art performance in detecting presentation attacks 
                                while maintaining computational efficiency for real-time applications.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Code</a>
                                <a href="#" class="publication-link">Demo</a>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Heritage Classification]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">MonuNet: A High Performance Deep Learning Network for Kolkata Heritage Image Classification</h3>
                            <p class="publication-authors">A Sasithradevi, B Chanthini, T Subbulakshmi, P Prakash</p>
                            <p class="publication-venue">Heritage Science, Springer, Vol. 12, pp. 1-14, 2024</p>
                            <p class="publication-description">
                                MonuNet introduces a specialized deep learning architecture for classifying Kolkata heritage monuments and architectural elements. 
                                Our network achieves superior performance in heritage image recognition, contributing to digital preservation of Indian cultural assets 
                                and supporting heritage tourism and conservation efforts.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Dataset</a>
                                <a href="#" class="publication-link">Code</a>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Food Freshness AI]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Is Human Perception Reliable? Toward Illumination Robust Food Freshness Prediction from Food Appearance</h3>
                            <p class="publication-authors">D Wang, S Sethu, S Nathan, Z Li, VJ Hogan, C Ni, S Zhang, HS Seo</p>
                            <p class="publication-venue">Journal of Food Engineering, Vol. 381, 112179, 2024</p>
                            <p class="publication-description">
                                We develop an illumination-robust AI system for predicting food freshness from visual appearance, using lettuce as a case study. 
                                Our approach outperforms human perception in consistency and accuracy, offering practical applications for food quality assessment 
                                in retail and agricultural industries.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Dataset</a>
                                <a href="#" class="publication-link">Code</a>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Tamil Heritage Art]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">KolamNetV2: Efficient Attention-Based Deep Learning Network for Tamil Heritage Art-Kolam Classification</h3>
                            <p class="publication-authors">A Sasithradevi, Sabarinathan, S Shoba, SMM Roomi, P Prakash</p>
                            <p class="publication-venue">Heritage Science, Springer, Vol. 12, pp. 60, 2024</p>
                            <p class="publication-description">
                                KolamNetV2 presents an enhanced attention-based architecture for classifying traditional Tamil Kolam art patterns. 
                                This work advances digital preservation of South Indian cultural heritage through improved accuracy and efficiency 
                                in recognizing complex geometric patterns and artistic traditions.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Dataset</a>
                                <a href="#" class="publication-link">Code</a>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Railroad UAV System]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Lightweight Railroad Semantic Segmentation Network and Distance Estimation for Railroad UAV Images</h3>
                            <p class="publication-authors">RS Rampriya, S Nathan, R Suganya, SB Prathiba, PS Perumal, W Wang</p>
                            <p class="publication-venue">Engineering Applications of Artificial Intelligence, Vol. 134, 108620, 2023</p>
                            <p class="publication-description">
                                We develop a lightweight neural network for real-time railroad semantic segmentation and distance estimation from UAV imagery. 
                                Our system enables automated railway infrastructure monitoring and maintenance planning, contributing to safer and more efficient 
                                railway operations through computer vision.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Code</a>
                                <a href="#" class="publication-link">Dataset</a>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Mobile Camera Enhancement]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Real-Time Under-Display Cameras Image Restoration and HDR on Mobile Devices</h3>
                            <p class="publication-authors">MV Conde, F Vasluianu, S Nathan, R Timofte</p>
                            <p class="publication-venue">European Conference on Computer Vision (ECCV), pp. 747-762, 2022</p>
                            <p class="publication-description">
                                We present a real-time solution for under-display camera image restoration and HDR processing optimized for mobile devices. 
                                Our approach addresses the unique challenges of under-display camera technology, enabling high-quality imaging through 
                                efficient deep learning methods suitable for smartphone deployment.
                            </p>
                            <div class="publication-links">
                                <a href="#" class="publication-link">Paper</a>
                                <a href="#" class="publication-link">Code</a>
                                <a href="#" class="publication-link">Demo</a>
                            </div>
                        </div>
                    </div>
                </article>

            </div>
        </section>

        <!-- News Section -->
        <section id="news" class="section">
            <h2 class="section-title">News</h2>
            <div class="news-grid">
                <div class="news-item">
                    <div class="news-date">Jan 2025</div>
                    <div class="news-content">ü•á WON 1st Place in Synthetic Dataset Track at 9th Sclera Segmentation and Benchmarking Competition (SSBC 2025)!</div>
                </div>
                <div class="news-item">
                    <div class="news-date">Jun 2024</div>
                    <div class="news-content">üèÜ Secured 7th place in Demosaic for Hybrid-EVS Camera competition at MIPI@CVPR 2024!</div>
                </div>
                <div class="news-item">
                    <div class="news-date">Nov 2024</div>
                    <div class="news-content">Face anti-spoofing paper accepted at CVPR 2024! Multiattention-Net achieves state-of-the-art performance</div>
                </div>
                <div class="news-item">
                    <div class="news-date">Oct 2024</div>
                    <div class="news-content">Two heritage preservation papers published in Heritage Science: MonuNet for Kolkata monuments and KolamNetV2</div>
                </div>
                <div class="news-item">
                    <div class="news-date">Sep 2024</div>
                    <div class="news-content">Food freshness prediction research published in Journal of Food Engineering - breakthrough in AI-assisted food quality assessment</div>
                </div>
            </div>
        </section>

        <!-- Education Section -->
        <section class="section">
            <h2 class="section-title">Education</h2>
            <div class="publications-grid">
                
                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [Deakin University]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Master of Data Science (Global)</h3>
                            <p class="publication-authors">Deakin University, Australia</p>
                            <p class="publication-venue">Graduate Degree ‚Ä¢ Data Science Specialization</p>
                            <p class="publication-description">
                                Comprehensive graduate program focusing on advanced data science methodologies, machine learning algorithms, 
                                statistical analysis, and big data technologies. Global program format providing international perspective 
                                on data science applications across various industries and research domains.
                            </p>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [UT McCombs]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Postgraduate Program in Artificial Intelligence & Machine Learning</h3>
                            <p class="publication-authors">Texas McCombs School of Business, University of Texas at Austin</p>
                            <p class="publication-venue">Postgraduate Certificate ‚Ä¢ <strong>GPA: 3.82/4.0</strong></p>
                            <p class="publication-description">
                                Advanced postgraduate program at one of the top business schools focusing on AI and ML applications 
                                in business contexts. Curriculum covered deep learning, computer vision, natural language processing, 
                                and strategic implementation of AI technologies in enterprise environments.
                            </p>
                        </div>
                    </div>
                </article>

                <article class="publication">
                    <div class="publication-header">
                        <div class="publication-image">
                            [TCE Engineering]
                        </div>
                        <div class="publication-info">
                            <h3 class="publication-title">Bachelor of Engineering (B.E.) - Electronics and Communications Engineering</h3>
                            <p class="publication-authors">Thiagarajar College of Engineering, India</p>
                            <p class="publication-venue">Undergraduate Degree ‚Ä¢ <strong>CGPA: 8.78/10.0</strong></p>
                            <p class="publication-description">
                                Strong foundational engineering degree with focus on electronics, communications systems, signal processing, 
                                and digital systems. Excellent academic performance providing solid technical foundation for advanced 
                                computer vision and machine learning research.
                            </p>
                        </div>
                    </div>
                </article>

            </div>
        </section>
        <section class="section">
            <h2 class="section-title">Selected Additional Publications</h2>
            <div class="service-grid">
                <div class="service-card">
                    <h3>Medical & Agricultural AI</h3>
                    <ul class="service-list">
                        <li><strong>SIMPD Net:</strong> South Indian Medicinal Plants Dataset (IEEE SILCON 2024)</li>
                        <li><strong>Polyp Segmentation:</strong> Multi-supervision Net with Attention (MAI 2022)</li>
                        <li><strong>Pesticide Residue Estimation:</strong> 3D Squeeze Excitation-Residual Network (ICCVG 2022)</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Heritage & Cultural Preservation</h3>
                    <ul class="service-list">
                        <li><strong>KolamNet:</strong> Attention-based Tamil Kolam Classification (ICCVG 2022)</li>
                        <li><strong>MonuNet:</strong> Kolkata Heritage Image Classification (Heritage Science 2024)</li>
                        <li><strong>KolamNetV2:</strong> Enhanced Tamil Heritage Art Classification (Heritage Science 2024)</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Computer Vision & Mobile Applications</h3>
                    <ul class="service-list">
                        <li><strong>Depth-guided Relighting:</strong> Lightweight Deep Learning Method (Journal of Imaging 2023)</li>
                        <li><strong>Instagram Filter Removal:</strong> Recurrent Residual Network (ICCVG 2022)</li>
                        <li><strong>Under-display Cameras:</strong> Real-time Image Restoration (ECCV 2022)</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Transportation & Infrastructure</h3>
                    <ul class="service-list">
                        <li><strong>Railroad Obstacle Detection:</strong> Deep Neural Networks (Applied AI 2022)</li>
                        <li><strong>Railroad Segmentation:</strong> Modified UNet Architecture (ICCIDE 2021)</li>
                        <li><strong>UAV Railroad Analysis:</strong> Lightweight Segmentation Network (Eng. Apps. AI 2023)</li>
                    </ul>
                </div>
            </div>
        </section>
        <!-- Academic Service Section -->
        <section id="service" class="section">
            <h2 class="section-title">Academic Service</h2>
            <div class="service-grid">
                <div class="service-card">
                    <h3>Research Specializations</h3>
                    <ul class="service-list">
                        <li>Biometric Systems & Sclera Segmentation</li>
                        <li>Cultural Heritage & Art Classification</li>
                        <li>Face Anti-Spoofing & Biometric Security</li>
                        <li>Food Technology & Agricultural AI</li>
                        <li>UAV-based Infrastructure Monitoring</li>
                        <li>Mobile Camera Enhancement Technologies</li>
                        <li>Medical Image Analysis & Segmentation</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Reviewer</h3>
                    <ul class="service-list">
                        <li>CVPR, ICCV, ECCV</li>
                        <li>NeurIPS, ICML, ICLR</li>
                        <li>TPAMI, IJCV</li>
                        <li>Nature Machine Intelligence</li>
                        <li>IEEE TIP, IEEE TMM</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Workshops</h3>
                    <ul class="service-list">
                        <li>CVPR Workshop on Computational Photography</li>
                        <li>ICCV Workshop on Mobile AI</li>
                        <li>NeurIPS Workshop on ML for Systems</li>
                        <li>ECCV Workshop on Advances in Image Manipulation</li>
                    </ul>
                </div>
                <div class="service-card">
                    <h3>Competition Awards & Recognition</h3>
                    <ul class="service-list">
                        <li><strong>ü•á 1st Place:</strong> <a href="https://sites.google.com/hyderabad.bits-pilani.ac.in/ssbc2025/home" target="_blank" style="color: #1a73e8;">Synthetic Dataset Track, 9th Sclera Segmentation and Benchmarking Competition (SSBC 2025)</a></li>
                        <li><strong>7th Place:</strong> Demosaic for Hybrid-EVS Camera, MIPI@CVPR 2024</li>
                        <li><strong>2nd Place:</strong> <a href="http://vcipl-okstate.org/pbvs/20/challenge.html" target="_blank" style="color: #1a73e8;">CVPR 2020 Thermal Image Super-Resolution Challenge</a></li>
                        <li><strong>2nd Place:</strong> <a href="https://drive.google.com/file/d/1EanMP-NBxJA5ZIH_HkiphTs2_BCcpHMJ/view?usp=sharing" target="_blank" style="color: #1a73e8;">IDD 2019 Challenge India Driving Dataset (NCVPRIPG 2019)</a></li>
                        <li><strong>3rd Place:</strong> <a href="https://research.fb.com/programs/openeds-challenge/#Announcement_of_the_Challenge_Winners" target="_blank" style="color: #1a73e8;">Facebook OpenEDS Segmentation Competition 2019</a></li>
                        <li><strong>3rd Place:</strong> <a href="http://ubee.enseeiht.fr/skelneton/index.html" target="_blank" style="color: #1a73e8;">CVPR 2019 Skeleton Segmentation Challenge</a></li>
                        <li><strong>Top Solver:</strong> <a href="https://drive.google.com/file/d/1t-M0gISoVtxxY35g8QuQ7FU8K2IX-g_K/view?usp=sharing" target="_blank" style="color: #1a73e8;">CrowdANALYTIX Background Removal Contest</a></li>
                        <li><strong>Kaggle Gold (Top 1%):</strong> <a href="https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting" target="_blank" style="color: #1a73e8;">14th Rank in Recruit Restaurant Visitor Forecasting</a></li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Contact Section -->
        <section id="contact" class="section">
            <h2 class="section-title">Contact</h2>
            <div class="contact-info">
                <h3>Get in Touch</h3>
                <p>Email: <a href="mailto:your.email@domain.com" class="contact-email">your.email@domain.com</a></p>
                <p>Office: Room 123, Computer Science Building</p>
                <p>University/Institution Name</p>
                <p>City, Country</p>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p class="footer-note">
                ¬© 2024 Sabari Nathan. All rights reserved. | Last updated: December 2024
            </p>
        </div>
    </footer>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Highlight active navigation item
        window.addEventListener('scroll', function() {
            const sections = document.querySelectorAll('section[id]');
            const navLinks = document.querySelectorAll('.nav-links a[href^="#"]');
            
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                const sectionHeight = section.clientHeight;
                if (scrollY >= (sectionTop - 200)) {
                    current = section.getAttribute('id');
                }
            });

            navLinks.forEach(link => {
                link.style.color = '#5f6368';
                if (link.getAttribute('href') === '#' + current) {
                    link.style.color = '#1a73e8';
                }
            });
        });
    </script>
</body>
</html>